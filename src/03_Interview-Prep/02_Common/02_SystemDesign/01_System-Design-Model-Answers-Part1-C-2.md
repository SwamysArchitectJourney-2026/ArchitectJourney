---
content_type: "interview_prep"
learning_level: "Intermediate"
prerequisites: ["System design basics", "TPM interview preparation"]
estimated_time: "25 minutes"
learning_objectives:
  - "Review system design model answers for event-driven systems, analytics, and CI/CD"
  - "Understand microservices patterns and stream processing architectures"
  - "Prepare for TPM system design interviews"
related_topics:
  prerequisites: ["./01_System-Design-Model-Answers-Part1-B.md"]
  builds_upon: []
  enables: ["./01_System-Design-Model-Answers-Part1-D.md"]
  cross_refs: []
---

**Data retention*** Hot layer: last X days in log-structured DB for fast queries.* Cold layer: S3 for long-term analytics.**Fault tolerance*** Kafka consumer group rebalancing, processing checkpoints, idempotent sinks.**APIs*** `GET /analytics/real-time?metric=ctr&window=5m`**Security*** Kafka ACLs, encryption in transit, role-based access to dashboards.**Testing*** Simulate high throughput using producer replay; test windowing correctness.**TPM delivery*** Teams: Ingest, Stream Processing, Serving, BI.* Sprint 0 for infra (Kafka cluster sizing, schema registry).* Deploy simple pipelines first (pageviews â†’ counts), then advanced (join streams, ML models).**Risks*** Hot partitions â†’ design partition keys carefully.* Schema drift â†’ enforce backward/forward compatibility rules.**Metrics*** Throughput (msgs/sec), processing lag, end-to-end latency, data loss (zero tolerance).---## 8) Scalable CI/CD Pipeline for 50+ Microservices**Requirements*** F: Reliable automated builds, tests, security scans, deploys; fast feedback; isolation per service; shared components.* NF: Scalable parallel builds, reproducible artifacts, traceable promotions.**Architecture & components*** Source code repos (mono-repo or many repos) â†’ CI runner pool (k8s or self-hosted runners) â†’ Build cache (artifact storage e.g., Nexus/Artifactory) â†’ Test stages (unit, integration, contract) â†’ Security scans (SCA/ SAST) â†’ CD orchestrator (ArgoCD/Spinnaker) â†’ Environment namespaces per team.**Design choices*** Prefer **trunk-based development** with feature flags.* Shared pipeline templates (YAML) for consistency.* Use ephemeral test environments (review apps) via namespaces.**Optimizations*** Build cache & incremental builds.* Parallel test execution per PR.* Selective pipeline execution (only affected services).**Observability*** Pipeline dashboards, SLAs for build times, failure rates.**Security*** Secrets in vault, image signing, policy-as-code.**TPM delivery*** Phase 1: infra + shared pipeline templates + 10 pilot services.* Phase 2: rollout to all services + performance tuning.**Risks*** CI congestion â†’ scale runners, prioritize pipelines.* Flaky tests â†’ quarantine, test stability metrics.**Metrics*** Mean time to feedback, build queue time, pipeline success rate, deployment frequency.---## ðŸ”— Related Documents- [Part 1-A: Ticket Booking and URL Shortener](./01_System-Design-Model-Answers-Part1-A-1.md) - Scenarios 1-2- [Part 1-B: HR Attendance, Meeting Scheduling, Payment Processing](./01_System-Design-Model-Answers-Part1-B-1.md) - Scenarios 3-5- [Part 1-D: Migration and Multi-Region](./01_System-Design-Model-Answers-Part1-D.md) - Scenarios 9-10- [System Design Questions](../../04_TPM/04_System-Design-Questions-Part1-A-1.md) - TPM-focused system design questions
